name: 2. Create VPC + EKS

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'Type: CREATE INFRA'
        required: true

jobs:
  create-infra:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    if: github.event.inputs.confirm == 'CREATE INFRA'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.0

      # ————————————————————————
      # 1. CLEAN LOCAL STATE (CRITICAL)
      # ————————————————————————
      - name: Clean Local Terraform State
        run: |
          cd infra/terraform/app
          rm -rf .terraform* *.tfstate *.tfstate.backup
          echo "Local state deleted — will use S3"

      # ————————————————————————
      # 2. INIT WITH S3 BACKEND
      # ————————————————————————
      - name: Terraform Init (S3 Backend)
        run: |
          cd infra/terraform/app

          echo "Initializing with S3 backend..."
          terraform init \
            -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
            -backend-config="key=eks/app.tfstate" \
            -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_TABLE }}" \
            -backend-config="region=${{ secrets.AWS_REGION }}" \
            -reconfigure

          echo "Backend configured:"
          terraform state list || echo "No state yet (first run)"

      # ————————————————————————
      # 3. PLAN & APPLY (FORCE)
      # ————————————————————————
      - name: Terraform Plan
        run: |
          cd infra/terraform/app
          terraform plan \
            -var="cluster_name=${{ secrets.EKS_CLUSTER_NAME }}" \
            -out=plan.tfplan

      - name: Terraform Apply
        run: |
          cd infra/terraform/app
          terraform apply -auto-approve plan.tfplan

          echo "Apply complete. Checking output..."
          terraform output -json > outputs.json
          cat outputs.json

      # ————————————————————————
      # 4. WAIT FOR EKS CLUSTER ACTIVE
      # ————————————————————————
      - name: Wait for EKS Cluster
        run: |
          echo "Waiting for cluster ${{ secrets.EKS_CLUSTER_NAME }} to be ACTIVE..."
          for i in {1..40}; do
            STATUS=$(aws eks describe-cluster \
              --name ${{ secrets.EKS_CLUSTER_NAME }} \
              --region ${{ secrets.AWS_REGION }} \
              --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")

            echo "Attempt $i: Status = $STATUS"

            if [[ "$STATUS" == "ACTIVE" ]]; then
              echo "EKS CLUSTER IS ACTIVE!"
              break
            fi

            if [[ "$STATUS" == "FAILED" ]]; then
              echo "CLUSTER CREATION FAILED"
              aws eks describe-cluster --name ${{ secrets.EKS_CLUSTER_NAME }} --region ${{ secrets.AWS_REGION }}
              exit 1
            fi

            sleep 30
          done

          if [[ "$STATUS" != "ACTIVE" ]]; then
            echo "TIMEOUT: Cluster not active after 20 minutes"
            exit 1
          fi

      # ————————————————————————
      # 5. RECONFIGURE & KUBECTL
      # ————————————————————————
      - name: Reconfigure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ secrets.EKS_CLUSTER_NAME }} --region ${{ secrets.AWS_REGION }} --alias techv
          kubectl get nodes

      # ————————————————————————
      # 6. FULL EBS CSI (YOUR ORIGINAL)
      # ————————————————————————
      - name: Create EBS CSI IAM Role
        run: |
          OIDC_ID=$(aws eks describe-cluster --name ${{ secrets.EKS_CLUSTER_NAME }} --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
          OIDC_URL="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:oidc-provider/oidc.eks.${{ secrets.AWS_REGION }}.amazonaws.com/id/$OIDC_ID"

          cat > ebs-trust-policy.json <<EOF
          {
            "Version": "2012-10-17",
            "Statement": [{
              "Effect": "Allow",
              "Principal": { "Federated": "$OIDC_URL" },
              "Action": "sts:AssumeRoleWithWebIdentity",
              "Condition": {
                "StringEquals": {
                  "oidc.eks.${{ secrets.AWS_REGION }}.amazonaws.com/id/$OIDC_ID:aud": "sts.amazonaws.com",
                  "oidc.eks.${{ secrets.AWS_REGION }}.amazonaws.com/id/$OIDC_ID:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa"
                }
              }
            }]
          }
          EOF

          ROLE_NAME="AmazonEKS_EBS_CSI_DriverRole_${{ secrets.EKS_CLUSTER_NAME }}"
          ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query 'Role.Arn' --output text 2>/dev/null || echo "")

          if [[ -z "$ROLE_ARN" ]]; then
            echo "Creating new IAM role..."
            ROLE_ARN=$(aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document file://ebs-trust-policy.json --query 'Role.Arn' --output text)
          else
            echo "Using existing role: $ROLE_ARN"
          fi

          aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy --role-name "$ROLE_NAME" || echo "Policy already attached"
          echo "ROLE_ARN=$ROLE_ARN" >> $GITHUB_ENV

      - name: Install EBS CSI Driver
        run: |
          aws eks create-addon --cluster-name ${{ secrets.EKS_CLUSTER_NAME }} --addon-name aws-ebs-csi-driver --service-account-role-arn ${{ env.ROLE_ARN }} --resolve-conflicts=OVERWRITE || true

          echo "Waiting 5 minutes for controller..."
          sleep 300

          echo "Checking EBS CSI controller..."
          for i in {1..12}; do
            if kubectl wait --for=condition=Available deployment/ebs-csi-controller -n kube-system --timeout=10s >/dev/null 2>&1; then
              echo "EBS CSI DRIVER READY"
              break
            fi
            sleep 10
          done

      # ————————————————————————
      # 7. SUCCESS
      # ————————————————————————
      - name: Success
        run: |
          echo "EKS CLUSTER CREATED SUCCESSFULLY!"
          echo "Cluster: ${{ secrets.EKS_CLUSTER_NAME }}"
          echo "Run '3. Deploy App' next"