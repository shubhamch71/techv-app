name: 2. Create VPC + EKS

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'Type: CREATE INFRA'
        required: true
        default: ''

jobs:
  create-infra:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    if: github.event.inputs.confirm == 'CREATE INFRA'
    steps:
      # -------------------------------------------------
      # 0. Checkout & AWS auth
      # -------------------------------------------------
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.0

      # -------------------------------------------------
      # 1. CLEAN LOCAL STATE
      # -------------------------------------------------
      - name: Clean Local Terraform State
        run: |
          cd infra/terraform/app
          rm -rf .terraform* *.tfstate *.tfstate.backup
          echo "Local state deleted — will use S3 backend"

      # -------------------------------------------------
      # 2. INIT WITH S3 BACKEND
      # -------------------------------------------------
      - name: Terraform Init (S3 Backend)
        run: |
          cd infra/terraform/app
          echo "Initializing Terraform with S3 backend..."
          terraform init \
            -backend-config="bucket=${{ secrets.TF_BACKEND_BUCKET }}" \
            -backend-config="key=eks/app.tfstate" \
            -backend-config="dynamodb_table=${{ secrets.TF_BACKEND_TABLE }}" \
            -backend-config="region=${{ secrets.AWS_REGION }}" \
            -reconfigure

          echo "Backend configured. State will be stored in S3."

      # -------------------------------------------------
      # EMERGENCY: FORCE UNLOCK STALE LOCK
      # -------------------------------------------------
      - name: Emergency Force Unlock
        run: |
          cd infra/terraform/app
          echo "Forcing unlock of stale lock ID: 38697dc3-0fc0-5062-896e-34d710219009"
          terraform force-unlock -force 38697dc3-0fc0-5062-896e-34d710219009 || echo "Lock already gone or failed (safe to ignore)"
          echo "Lock cleared."

      # -------------------------------------------------
      # 3. TERRAFORM PLAN + APPLY (FULLY IDEMPOTENT)
      # -------------------------------------------------
      - name: Terraform Plan & Apply (Safe & Idempotent)
        id: terraform_apply
        run: |
          cd infra/terraform/app
          MAX_RETRIES=3
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "Attempt $((RETRY_COUNT + 1)) of $MAX_RETRIES..."

            # Refresh state
            echo "Refreshing Terraform state..."
            terraform refresh -var="cluster_name=${{ secrets.EKS_CLUSTER_NAME }}" || true

            # Run plan
            echo "Running terraform plan..."
            set +e
            terraform plan \
              -var="cluster_name=${{ secrets.EKS_CLUSTER_NAME }}" \
              -detailed-exitcode \
              > plan.log 2>&1
            PLAN_EXIT=$?
            set -e

            cat plan.log

            # === NO CHANGES ===
            if [ $PLAN_EXIT -eq 0 ]; then
              echo "No changes required. Infrastructure is up to date."
              break
            fi

            # === CHANGES → APPLY ===
            if [ $PLAN_EXIT -eq 2 ]; then
              echo "Changes detected. Running apply..."
              set +e
              terraform apply -auto-approve \
                -var="cluster_name=${{ secrets.EKS_CLUSTER_NAME }}" \
                > apply.log 2>&1
              TF_EXIT=$?
              set -e

              # === LOCK ERROR ===
              if grep -q "Error acquiring the state lock\|ConditionalCheckFailedException" apply.log; then
                echo "State lock detected."
                LOCK_ID=$(grep -o 'ID: [a-f0-9-]\+' apply.log | head -1 | awk '{print $2}')
                if [[ -n "$LOCK_ID" ]]; then
                  echo "Force unlocking lock ID: $LOCK_ID"
                  terraform force-unlock -force "$LOCK_ID" && echo "Unlocked" || echo "Unlock failed, retrying..."
                fi
                RETRY_COUNT=$((RETRY_COUNT + 1))
                sleep 10
                continue
              fi

              # === AlreadyExists → IMPORT ===
              if grep -q "EntityAlreadyExists\|AlreadyExistsException" apply.log; then
                echo "Detected existing resources. Importing..."

                # IAM User
                if grep -q "aws_iam_user.admin" apply.log; then
                  USER_NAME=$(grep -o "User with name [^ ]*" apply.log | awk '{print $4}')
                  echo "Importing IAM user: $USER_NAME"
                  terraform import aws_iam_user.admin "$USER_NAME" || true
                fi

                # KMS Key & Alias
                if grep -q 'aws_kms_alias.this\["cluster"\]' apply.log; then
                  ALIAS_NAME=$(grep -o 'alias/eks/[^ ]*' apply.log | head -1)
                  KEY_ID=$(aws kms list-aliases \
                    --query "Aliases[?AliasName=='arn:aws:kms:${{ secrets.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:$ALIAS_NAME'].TargetKeyId" \
                    --output text)
                  [[ -n "$KEY_ID" ]] && terraform import "module.eks_cluster.module.kms.aws_kms_key.this[\"cluster\"]" "$KEY_ID" || true
                  [[ -n "$ALIAS_NAME" ]] && terraform import "module.eks_cluster.module.kms.aws_kms_alias.this[\"cluster\"]" "$ALIAS_NAME" || true
                fi

                # CloudWatch Log Group
                if grep -q 'aws_cloudwatch_log_group.this\[0\]' apply.log; then
                  LOG_GROUP=$(grep -o '/aws/eks/[^ ]*/cluster' apply.log | head -1)
                  echo "Importing log group: $LOG_GROUP"
                  terraform import module.eks_cluster.aws_cloudwatch_log_group.this[0] "$LOG_GROUP" || true
                fi

                # EKS CLUSTER
                if ! terraform state list | grep -q "module.eks_cluster.aws_eks_cluster.this"; then
                  echo "Importing EKS cluster: ${{ secrets.EKS_CLUSTER_NAME }}"
                  terraform import module.eks_cluster.aws_eks_cluster.this ${{ secrets.EKS_CLUSTER_NAME }} || true
                fi

                echo "Re-applying after import..."
                terraform apply -auto-approve -var="cluster_name=${{ secrets.EKS_CLUSTER_NAME }}"
                break
              fi

              # === SUCCESS ===
              if [ $TF_EXIT -eq 0 ]; then
                echo "Terraform apply completed successfully."
                break
              else
                echo "Apply failed. Retrying..."
                cat apply.log
                RETRY_COUNT=$((RETRY_COUNT + 1))
                sleep 10
              fi
            else
              echo "Plan failed. Attempting cluster import..."
              terraform import module.eks_cluster.aws_eks_cluster.this ${{ secrets.EKS_CLUSTER_NAME }} || true
              terraform apply -auto-approve -var="cluster_name=${{ secrets.EKS_CLUSTER_NAME }}"
              break
            fi
          done

          if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
            echo "Max retries reached. Failing."
            cat plan.log apply.log 2>/dev/null || true
            exit 1
          fi

      # -------------------------------------------------
      # 4. CLEANUP ON FAILURE
      # -------------------------------------------------
      - name: Force Unlock on Final Failure
        if: failure()
        run: |
          cd infra/terraform/app
          echo "Final cleanup: unlocking any stale lock..."
          LOCK_ID=$(grep -o 'ID: [a-f0-9-]\+' apply.log plan.log 2>/dev/null | head -1 | awk '{print $2}' || echo "")
          [[ -n "$LOCK_ID" ]] && terraform force-unlock -force "$LOCK_ID" || true

      # -------------------------------------------------
      # 5. WAIT FOR EKS CLUSTER ACTIVE
      # -------------------------------------------------
      - name: Wait for EKS Cluster to be ACTIVE
        run: |
          echo "Waiting for EKS cluster '${{ secrets.EKS_CLUSTER_NAME }}' to become ACTIVE..."
          timeout=1200
          interval=30
          elapsed=0

          while [ $elapsed -lt $timeout ]; do
            STATUS=$(aws eks describe-cluster \
              --name ${{ secrets.EKS_CLUSTER_NAME }} \
              --region ${{ secrets.AWS_REGION }} \
              --query 'cluster.status' --output text 2>/dev/null || echo "NOT_FOUND")

            echo "Cluster status: $STATUS (elapsed: ${elapsed}s)"

            if [[ "$STATUS" == "ACTIVE" ]]; then
              echo "EKS CLUSTER IS ACTIVE!"
              break
            fi

            if [[ "$STATUS" == "FAILED" ]]; then
              echo "EKS CLUSTER CREATION FAILED"
              aws eks describe-cluster --name ${{ secrets.EKS_CLUSTER_NAME }} --region ${{ secrets.AWS_REGION }}
              exit 1
            fi

            sleep $interval
            elapsed=$((elapsed + interval))
          done

          if [[ "$STATUS" != "ACTIVE" ]]; then
            echo "TIMEOUT: Cluster did not become ACTIVE in 20 minutes"
            exit 1
          fi

      # -------------------------------------------------
      # 6. RE-AUTH & KUBECTL
      # -------------------------------------------------
      - name: Reconfigure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          echo "kubectl installed"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name ${{ secrets.EKS_CLUSTER_NAME }} \
            --region ${{ secrets.AWS_REGION }} \
            --alias techv
          echo "Connected to EKS"
          kubectl get nodes

      # -------------------------------------------------
      # 7. POD IDENTITY AGENT
      # -------------------------------------------------
      - name: Install Pod Identity Agent
        run: |
          echo "Installing eks-pod-identity-agent..."
          aws eks create-addon \
            --cluster-name ${{ secrets.EKS_CLUSTER_NAME }} \
            --addon-name eks-pod-identity-agent \
            --resolve-conflicts=OVERWRITE \
            || echo "Already installed"
          echo "Waiting 4 minutes..."
          sleep 240

      # -------------------------------------------------
      # 8. EBS CSI IAM ROLE (FULL LOGS)
      # -------------------------------------------------
      - name: Create EBS CSI IAM Role
        run: |
          echo "Setting up IAM role for EBS CSI driver..."

          OIDC_ID=$(aws eks describe-cluster \
            --name ${{ secrets.EKS_CLUSTER_NAME }} \
            --query "cluster.identity.oidc.issuer" \
            --output text | cut -d '/' -f 5)

          OIDC_URL="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:oidc-provider/oidc.eks.${{ secrets.AWS_REGION }}.amazonaws.com/id/$OIDC_ID"

          cat > ebs-trust-policy.json <<EOF
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": { "Federated": "$OIDC_URL" },
                "Action": "sts:AssumeRoleWithWebIdentity",
                "Condition": {
                  "StringEquals": {
                    "oidc.eks.${{ secrets.AWS_REGION }}.amazonaws.com/id/$OIDC_ID:aud": "sts.amazonaws.com",
                    "oidc.eks.${{ secrets.AWS_REGION }}.amazonaws.com/id/$OIDC_ID:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa"
                  }
                }
              }
            ]
          }
          EOF

          ROLE_NAME="AmazonEKS_EBS_CSI_DriverRole_${{ secrets.EKS_CLUSTER_NAME }}"
          ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query 'Role.Arn' --output text 2>/dev/null || echo "")

          if [[ -z "$ROLE_ARN" ]]; then
            echo "Creating new IAM role..."
            ROLE_ARN=$(aws iam create-role \
              --role-name "$ROLE_NAME" \
              --assume-role-policy-document file://ebs-trust-policy.json \
              --query 'Role.Arn' --output text)
            echo "Created role: $ROLE_ARN"
          else
            echo "Using existing role: $ROLE_ARN"
          fi

          echo "Attaching AmazonEBSCSIDriverPolicy..."
          aws iam attach-role-policy \
            --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
            --role-name "$ROLE_NAME" \
            || echo "Policy already attached"

          echo "ROLE_ARN=$ROLE_ARN" >> $GITHUB_ENV

      # -------------------------------------------------
      # 9. INSTALL EBS CSI v1.42.0 + FULL WAIT
      # -------------------------------------------------
      - name: Install EBS CSI Driver v1.42.0
        run: |
          echo "Installing aws-ebs-csi-driver add-on..."
          aws eks create-addon \
            --cluster-name ${{ secrets.EKS_CLUSTER_NAME }} \
            --addon-name aws-ebs-csi-driver \
            --addon-version v1.42.0 \
            --service-account-role-arn ${{ env.ROLE_ARN }} \
            --resolve-conflicts=OVERWRITE \
            || echo "Already installed"

          echo "Waiting 5 minutes for controller to start..."
          sleep 300

          echo "Checking EBS CSI controller deployment..."
          for i in {1..12}; do
            if kubectl wait --for=condition=Available deployment/ebs-csi-controller -n kube-system --timeout=10s >/dev/null 2>&1; then
              echo "EBS CSI DRIVER READY"
              break
            fi
            echo "Controller not ready yet (attempt $i/12)..."
            sleep 10
          done

          if ! kubectl get deployment ebs-csi-controller -n kube-system >/dev/null 2>&1; then
            echo "EBS CSI controller deployment missing!"
            exit 1
          fi

      # -------------------------------------------------
      # 10. SUCCESS
      # -------------------------------------------------
      - name: Success
        run: |
          echo "EKS INFRA CREATION COMPLETE!"
          echo "Cluster: ${{ secrets.EKS_CLUSTER_NAME }}"
          echo "Region: ${{ secrets.AWS_REGION }}"
          echo "Next: Run '3. Deploy App'"